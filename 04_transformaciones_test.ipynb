{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d9b19c-f093-4552-bb96-4007635e67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/14 15:33:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros cargados para validación: 967\n",
      " Columnas encontradas: 16\n",
      "root\n",
      " |-- referencia_del_contrato: string (nullable = true)\n",
      " |-- nit_entidad: string (nullable = true)\n",
      " |-- nombre_entidad: string (nullable = true)\n",
      " |-- departamento: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- tipo_de_contrato: string (nullable = true)\n",
      " |-- valor_del_contrato: double (nullable = true)\n",
      " |-- fecha_de_firma: string (nullable = true)\n",
      " |-- estado_contrato: string (nullable = true)\n",
      " |-- departamento_idx: double (nullable = true)\n",
      " |-- tipo_de_contrato_idx: double (nullable = true)\n",
      " |-- estado_contrato_idx: double (nullable = true)\n",
      " |-- departamento_vec: vector (nullable = true)\n",
      " |-- tipo_de_contrato_vec: vector (nullable = true)\n",
      " |-- estado_contrato_vec: vector (nullable = true)\n",
      " |-- features_raw: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MUESTRA DE DATOS ESCALADOS Y LISTOS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/14 15:33:31 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-------------------------------------------------------------------------------------------------+------------------+\n",
      "|features_raw                               |features_scaled                                                                                  |valor_del_contrato|\n",
      "+-------------------------------------------+-------------------------------------------------------------------------------------------------+------------------+\n",
      "|(53,[0,32,48,52],[1.0,1.0,1.0,7.3451015E7])|(53,[0,32,48,52],[2.115887659944155,2.939577643658159,11.790091178612657,0.0012021355132655794]) |7.3451015E7       |\n",
      "|(53,[26,32,43,52],[1.0,1.0,1.0,6673341.0]) |(53,[26,32,43,52],[13.935715195287179,2.939577643658159,2.340529601977721,1.0921918789319976E-4])|6673341.0         |\n",
      "|(53,[0,32,42,52],[1.0,1.0,1.0,1.32E7])     |(53,[0,32,42,52],[2.115887659944155,2.939577643658159,2.127481987864246,2.1603770587929447E-4])  |1.32E7            |\n",
      "|(53,[0,32,42,52],[1.0,1.0,1.0,6.75268E7])  |(53,[0,32,42,52],[2.115887659944155,2.939577643658159,2.127481987864246,0.001105176890709844])   |6.75268E7         |\n",
      "|(53,[16,32,44,52],[1.0,1.0,1.0,1.5E7])     |(53,[16,32,44,52],[8.678725190029034,2.939577643658159,2.6897074231791147,2.454973930446528E-4]) |1.5E7             |\n",
      "+-------------------------------------------+-------------------------------------------------------------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¡Validación completa! Datos guardados en: /opt/spark-data/processed/secop_final_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Iniciar Sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_Validacion_Transformacion\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Cargar los datos que generamos en el Notebook 03\n",
    "# Esta es la ruta donde guardamos el archivo final procesado\n",
    "path_features = \"/opt/spark-data/processed/secop_features.parquet\"\n",
    "df_transformed = spark.read.parquet(path_features)\n",
    "\n",
    "print(f\" Registros cargados para validación: {df_transformed.count():,}\")\n",
    "print(f\" Columnas encontradas: {len(df_transformed.columns)}\")\n",
    "\n",
    "# 3. Mostrar la estructura de los datos (Esquema)\n",
    "# Aquí verificamos que existan las columnas _idx, _vec y features_raw\n",
    "df_transformed.printSchema()\n",
    "\n",
    "# 4. RETO: Aplicar Escalado (Opcional pero recomendado para Regresión)\n",
    "# El StandardScaler hace que todas las variables numéricas estén en la misma escala\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\", \n",
    "    outputCol=\"features_scaled\", \n",
    "    withStd=True, \n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Entrenamos el escalador con los datos cargados\n",
    "scaler_model = scaler.fit(df_transformed)\n",
    "df_final = scaler_model.transform(df_transformed)\n",
    "\n",
    "# 5. Ver el resultado final antes de la Regresión\n",
    "print(\"\\n=== MUESTRA DE DATOS ESCALADOS Y LISTOS ===\")\n",
    "df_final.select(\"features_raw\", \"features_scaled\", \"valor_del_contrato\").show(5, truncate=False)\n",
    "\n",
    "# Guardar el dataset final (ya escalado) para el Notebook 05\n",
    "output_final = \"/opt/spark-data/processed/secop_final_ready.parquet\"\n",
    "df_final.write.mode(\"overwrite\").parquet(output_final)\n",
    "\n",
    "print(f\" ¡Validación completa! Datos guardados en: {output_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
