{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a41ab5-1034-4468-93d9-6fe248db5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros iniciales: 1,000\n",
      "Registros después de limpieza: 967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "¡PROCESO COMPLETADO EXITOSAMENTE!\n",
      "Dataset guardado en: /opt/spark-data/processed/secop_features.parquet\n",
      "========================================\n",
      "+-------------------------------------------+------------------+\n",
      "|features_raw                               |valor_del_contrato|\n",
      "+-------------------------------------------+------------------+\n",
      "|(53,[0,32,48,52],[1.0,1.0,1.0,7.3451015E7])|7.3451015E7       |\n",
      "|(53,[26,32,43,52],[1.0,1.0,1.0,6673341.0]) |6673341.0         |\n",
      "|(53,[0,32,42,52],[1.0,1.0,1.0,1.32E7])     |1.32E7            |\n",
      "|(53,[0,32,42,52],[1.0,1.0,1.0,6.75268E7])  |6.75268E7         |\n",
      "|(53,[16,32,44,52],[1.0,1.0,1.0,1.5E7])     |1.5E7             |\n",
      "+-------------------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Configurar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_FeatureEngineering\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Cargar datos desde la ruta del Notebook 01\n",
    "path_input = \"/opt/spark-data/raw/secop_contratos.parquet\"\n",
    "df = spark.read.parquet(path_input)\n",
    "print(f\"Registros iniciales: {df.count():,}\")\n",
    "\n",
    "# --- SOLUCIÓN AL ERROR DE TIPO ---\n",
    "# Convertimos el valor del contrato de Texto a Número (Double)\n",
    "# Esto es obligatorio para que el modelo pueda hacer cálculos matemáticos\n",
    "df = df.withColumn(\"valor_del_contrato\", col(\"valor_del_contrato\").cast(\"double\"))\n",
    "\n",
    "# 3. Limpieza de datos (Reto 2)\n",
    "# Filtramos departamentos, tipo, estado y valor para que no tengan nulos ni ceros\n",
    "categorical_cols = [\"departamento\", \"tipo_de_contrato\", \"estado_contrato\"]\n",
    "df_clean = df.dropna(subset=categorical_cols + [\"valor_del_contrato\"]) \\\n",
    "             .filter(col(\"valor_del_contrato\") > 0)\n",
    "\n",
    "print(f\"Registros después de limpieza: {df_clean.count():,}\")\n",
    "\n",
    "# 4. Construcción del Pipeline de IA\n",
    "# Paso A: StringIndexer (Letras a números de índice)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") \n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# Paso B: OneHotEncoder (Índices a vectores binarios)\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_vec\") \n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# Paso C: VectorAssembler (Unir todo en una sola columna 'features')\n",
    "# Usamos las versiones _vec y el valor numérico\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_vec\" for c in categorical_cols] + [\"valor_del_contrato\"],\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# 5. Ejecutar el proceso completo\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "# 6. Guardar los resultados para el Notebook 05 de Regresión\n",
    "output_path = \"/opt/spark-data/processed/secop_features.parquet\"\n",
    "pipeline_path = \"/opt/spark-data/processed/feature_pipeline\"\n",
    "\n",
    "df_transformed.write.mode(\"overwrite\").parquet(output_path)\n",
    "pipeline_model.write().overwrite().save(pipeline_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"¡PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"Dataset guardado en: {output_path}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Mostrar muestra del resultado final\n",
    "df_transformed.select(\"features_raw\", \"valor_del_contrato\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5051018-bedc-4291-8a62-86d364c37423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
